"""Load CSVs to PostgreSQL tables"""

def generate_sql_stmt(dataset, table_name, column_def, source_file=None):
    """Generates SQL commands to load CSV to Postgres.
    
    Parameters
    ----------
    dataset : str
        The name of the StackExchange dataset. This will be used as schema name.
    
    table_name : str
        The name of the dataset table to load to Postgres.
        
    column_def : dict
        A dictionary of column names and data types of the table to be created.
        
    source_file : str, default None
        The complete path to the source CSV file. By default, the path used comes
        from the metadata generated by the transform step in the ETL pipeline. When
        this is set, this value is used instead.
        
    Returns
    -------
    sql : str
        The complete SQL statement
    
    """
    
    import json
    import os
    from src.utils.misc import camel_to_snake_case
    

    dataset_metadata = json.load(open('data/metadata/{}.json'.format(dataset), 'r'))
    table_metadata = dataset_metadata[table_name]
    data_columns = table_metadata['columns']
    
    # Adjust column def based on fields present in actual data
    column_def_adj = {x:column_def[x] for x in data_columns if x in column_def.keys()}
    
    # Check if all data columns have corresponding column type in schema definition
    assert len([x for x in data_columns if x not in column_def.keys()])==0, 'Error. Extra columns are present in data which are missing in schema definition.'
    
    if source_file is not None:
        filepath = source_file
    else:
        filepath = table_metadata['csv']
        
    schema_name = camel_to_snake_case(dataset.replace('.','_'))
    
    # DROP table statement
    drop_stmt = 'DROP TABLE IF EXISTS {}.{};'.format(schema_name, table_name)
    
    # CREATE statement
    lines = []
    lines.append('CREATE TABLE {}.{} (\n'.format(schema_name, table_name))
    lines += ['    {}\t{},\n'.format(k.ljust(20,' '),v) for k,v in column_def_adj.items()]
    lines.append('    PRIMARY KEY (id)\n')
    lines.append(');')
    create_stmt = ''.join(lines)
    
    # COPY statement
    lines = []
    lines.append('COPY {}.{}({})'.format(schema_name, table_name, ', '.join(column_def_adj.keys())))
    lines.append("FROM '{}'".format(filepath))
    lines.append("WITH (")
    lines.append("  FORMAT CSV,")
    lines.append("  DELIMITER ',',")
    lines.append("  NULL '',")
    lines.append("  HEADER,")
    lines.append("  FORCE_NULL({})".format(', '.join([x for x in column_def_adj if x != 'id'])))
    lines.append(");")
    copy_stmt = '\n'.join(lines)
        
    sql = '\n\n'.join([drop_stmt, 
                       create_stmt, 
                       copy_stmt, 
                       'COMMIT;'])
    
    return sql


def load_data(dataset, table_name):
    """Loads CSV data to a Postgres table.
    
    Parameters
    ----------
    dataset : str
        The name of the StackExchange dataset. This will be used as schema name.
    
    table_name : str
        The name of the dataset table to load to Postgres.

    Returns
    -------
    None
    
    """
    
    import os
    import json
    from sqlalchemy import create_engine
    from src.utils.misc import camel_to_snake_case
    
    root = os.getcwd()
    
    # Parse schema info
    schema_def = json.load(open('{}/src/schema/postgres_schema.json'.format(root),'r'))
    column_def = schema_def[table_name]
    sql_stmt = generate_sql_stmt(dataset, table_name, column_def)
    
    # Execute SQL commands
    DATABASE_URI = 'postgres+psycopg2://postgres:password@localhost:5432/stackexchange'
    engine = create_engine(DATABASE_URI)
    
    with engine.connect() as conn:
        conn.execute(sql_stmt)
        
    print('Loaded data to: {}.{}'.format(camel_to_snake_case(dataset.replace('.','_')), table_name))
    
    
def load_dataset(dataset):
    """Loads all tables for a specific dataset.
    
    Parameters
    ----------
    dataset : str
        The name of the StackExchange dataset. This will be used as schema name.
        
    Returns
    -------
    None
    
    """
    
    import os
    import json
    
    
    # Parse schema info
    root = os.getcwd()
    metadata = json.load(open('{}/src/schema/postgres_schema.json'.format(root),'r'))
    table_list = list(metadata.keys())
    
    # Load each table data
    for table_name in table_list:
        load_data(dataset, table_name)
        
    print('\n\nFinished loading all tables for dataset: {}'.format(dataset))
    
    
def write_sql_statements(dataset):
    """Generates SQL statements into .sql files for Docker postgres initialization.
    
    Parameters
    ----------
    dataset : str
        The name of the StackExchange dataset. This will be used as schema name.
        
    Returns
    -------
    None
    
    """
    
    
    import os
    import json
    from src.utils.misc import camel_to_snake_case
    
    
    # Parse schema info
    root = os.getcwd()
    schema_def = json.load(open('{}/src/schema/postgres_schema.json'.format(root),'r'))
    schema_name = camel_to_snake_case(dataset.replace('.','_'))
    csv_source_dir = '/docker-entrypoint-initdb.d/data/'
    table_list = list(schema_def.keys())
    
    sql_list = ['BEGIN;','CREATE SCHEMA {};'.format(schema_name)]
    
    # Load each table data
    for table_name in table_list:
        column_def = schema_def[table_name]
        csv_source_file = '{}{}.csv'.format(csv_source_dir, table_name)
        stmt = generate_sql_stmt(dataset, table_name, column_def, source_file=csv_source_file)
        
        sql_list.append(stmt)
        
        
    sql_stmt = '\n\n'.join(sql_list)
    
    # Write to .sql file
    sql_file = 'docker/init.sql'
    with open(sql_file, 'w') as f:
        f.write(sql_stmt)
        


if __name__ == '__main__':
    
    import os
    import argparse
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(prog='load')
    parser.add_argument('--dataset', action='store', required=True, help='The dataset to ETL')
    parser.add_argument('--command', action='store', required=True, help='The ETL command to execute, e.g. write_sql, load')
    args = parser.parse_args()

    
    # Ensure that script is executed in the correct directory
    cwd = os.getcwd()
    assert cwd.split('/')[-1] == 'stack-exchange-dwh', 'Script was executed in the wrong directory: {}.'.format(cwd)
    
    
    # Check if the pipeline argument is valid
    assert args.command in ['write_sql', 'load'], 'Invalid pipeline argument specified: {}'.format(args.command)
    
    
    # Run pipeline
    if args.command == 'write_sql':
        write_sql_statements(dataset=args.dataset)
        
    elif args.command == 'load':
        load_dataset(dataset=args.dataset)
        